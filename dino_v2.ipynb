{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQmbIbATxU9Z",
        "outputId": "f2015975-64d1-4f73-9d05-742ff928afdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 17 14:44:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cEP1r2GtH0n",
        "outputId": "03ceec33-4545-47e4-adba-6cd6caed2d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 463, done.\u001b[K\n",
            "remote: Total 463 (delta 0), reused 0 (delta 0), pack-reused 463 (from 1)\u001b[K\n",
            "Receiving objects: 100% (463/463), 12.91 MiB | 14.43 MiB/s, done.\n",
            "Resolving deltas: 100% (220/220), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision transformers timm supervision pycocotools opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qA05uKktULm",
        "outputId": "f987809d-0967-42ca-e097-98ad1ddbab76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/212.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/GroundingDINO\")"
      ],
      "metadata": {
        "id": "nKBSSuYBtf8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install addict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8h23R3auE9s",
        "outputId": "4728f20e-ee5b-4d40-a0e1-110a81d1756e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Installing collected packages: addict\n",
            "Successfully installed addict-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yapf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU5DDuKxuI3A",
        "outputId": "5882d2c5-bceb-4506-dfa7-6b1410611fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yapf\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.12/dist-packages (from yapf) (4.5.1)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/256.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yapf\n",
            "Successfully installed yapf-0.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groundingdino.util.inference import load_model, load_image, predict\n",
        "print(\"GroundingDINO imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvUxXAKetgj5",
        "outputId": "3c64861a-b76d-4eeb-9581-acd17a015038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundingDINO imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n",
            "SyntaxWarning: invalid escape sequence '\\s'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_PATH = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzNeK-ybtjn6",
        "outputId": "f5f39229-9d38-4420-fca3-e3d9aeeb86f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py ; exist: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ],
      "metadata": {
        "id": "BJrtNaJvuS2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHTS_PATH = \"groundingdino_swint_ogc.pth\"\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niPs90UFudbN",
        "outputId": "a3f07f14-0faf-48ca-95ec-e037983a364c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groundingdino_swint_ogc.pth ; exist: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groundingdino.util.inference import load_model, load_image, predict, annotate"
      ],
      "metadata": {
        "id": "cyjKl4a6umy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "f9ab6c9fea1f4109a41bac2eaf558dad",
            "ebf1c8d34df14dcabca5d38ea61c91fa",
            "8b63ae7fb0a642a181ed4d0ef31dfbd5",
            "be97b96ff6ad404e8042e6e4f74d976e",
            "6d60f942b55a456aac740672a55983b5",
            "3a594b0163f54f80a115b04f1178df07",
            "b78baa402ebb49528c0d7d9625176a8e",
            "ee43b42fc16b4bceb0e927e8ed7ffef7",
            "1b733617ae994518ab1a08c8a592b516",
            "f7bb3ee350ea400a84bb8602b717821b",
            "6d1d66bc51694aa18b58d3183e1b3a4e",
            "38b8cdad004249248d5e090a3eb2f038",
            "1808758727dc4ca781d239eb47c855a8",
            "09abe73199d341e7957d06d2706984be",
            "830c93fbaee94580be11dfd3c709ed4d",
            "e0a3386ac310476dba0f023db017632b",
            "844062a0933d4f71bca19b3750ea7a93",
            "3eca3990d8a5419ab896dedc8ffe7b59",
            "96efbe1503204935afaa39c893985a5d",
            "85997acbe109450bb32ae3e260a99496",
            "4c9cedbe659d4164adb0d820a11f1f1a",
            "49fb0be3fe344256a584d2a206d197ba",
            "d2fb23f7444b4e619ab49885f1e7e2ac",
            "69ad0219516e440ab0b70790a2762346",
            "b4e66feb7f55492c99f7b53bad430d92",
            "247eaa3456f94d2ab92fd3a37fb31278",
            "6394419a6f034578bbdf6f690f418b7c",
            "7c831fc66236451aa81b47d929e77efb",
            "46c8ce1e20be4b9fa00f1f1e3a55652a",
            "4c66160d4afc4df9bdd9041f007c7198",
            "2089589bc7f84d679f6fc8bed3411e9f",
            "c2fc123deda54fca814f666fab1e03da",
            "0d3df9fde7b54b0b8c704f6e256dca82",
            "64b12ff9473e4a0a8e134b3a28093a9f",
            "5ee9399c7c1844399011ab355b7de773",
            "d46e5dafa4e849eeb8888ebc9dab4007",
            "0760072198904278a91d7edf52e8e5ad",
            "8ccad626cdde4b5d8bf1fb23379792a6",
            "e546a22d786a4f30953eb0cc2681e7b8",
            "85bf806cb2504e449ce264696262da5b",
            "f4f8128424674bdeb411a8853019f667",
            "feb6e6c75e814cfb8fafa1e0dc2b541a",
            "da5a2a25735543d88635b85b3e93e972",
            "2930a107ec664d0ebcaf78364d2c9147",
            "c72dc7c34d1c497499faadb59405f7b2",
            "b8a76dbab8d34eaab24adcae2ed2d8aa",
            "1751e7d562fb4450bfb29d6f96d39953",
            "2cbee124f20b4d8b8793ab33fe06546b",
            "0132eb95c93c4042bcc12d00b7923dfa",
            "026c28f3f07d4985b8986edd4278b32a",
            "bac1493e48df43f0b363e49cd08935c9",
            "83937be1efac488c8ccb8f2cd10e3f09",
            "70032fc09bea46bf94510e742487140d",
            "ac71715115df4a148a8e3c74609408ea",
            "ebce2c1b1d414320be1f8b3f2bd2716d"
          ]
        },
        "id": "WWonX75euvYE",
        "outputId": "8bcb7626-c600-4e27-c290-bf974238c2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final text_encoder_type: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9ab6c9fea1f4109a41bac2eaf558dad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38b8cdad004249248d5e090a3eb2f038"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2fb23f7444b4e619ab49885f1e7e2ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64b12ff9473e4a0a8e134b3a28093a9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c72dc7c34d1c497499faadb59405f7b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHFCIghovDLY",
        "outputId": "1b78d08c-26e3-4170-c045-5f0e41eb9b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_PATH = \"/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "CHECKPOINT_PATH = \"/content/groundingdino_swint_ogc.pth\"\n",
        "\n",
        "model = load_model(\n",
        "    CONFIG_PATH,\n",
        "    CHECKPOINT_PATH,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded on CPU successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RBy4Ia1vGVT",
        "outputId": "bd8abcaa-efba-46ad-c4bb-c36b501a1d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final text_encoder_type: bert-base-uncased\n",
            "Model loaded on CPU successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = \"/content/aphid1.jpg\"\n",
        "\n",
        "TEXT_PROMPT = \"aphid\"\n",
        "BOX_THRESHOLD = 0.35\n",
        "TEXT_THRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)"
      ],
      "metadata": {
        "id": "mKGx2MqTvRH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxes, logits, phrases = predict(\n",
        "    model=model,\n",
        "    image=image,\n",
        "    caption=TEXT_PROMPT,\n",
        "    box_threshold=BOX_THRESHOLD,\n",
        "    text_threshold=TEXT_THRESHOLD,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz-f24aUvYJk",
        "outputId": "355c855b-b95b-4d6f-a171-dac9be9371c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_frame = annotate(\n",
        "    image_source=image_source,\n",
        "    boxes=boxes,\n",
        "    logits=logits,\n",
        "    phrases=phrases\n",
        ")\n",
        "\n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "Ff1AcRQIvZNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for u in ['aphid', 'whitefly']:\n",
        "  for i in range(1, 5):\n",
        "    IMAGE_PATH = f\"/content/{u}{i}.jpg\"\n",
        "\n",
        "\n",
        "    TEXT_PROMPT = f\"{u}\"\n",
        "    BOX_TRESHOLD = 0.35\n",
        "    TEXT_TRESHOLD = 0.25\n",
        "\n",
        "    image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "    boxes, logits, phrases = predict(\n",
        "        model=model,\n",
        "        image=image,\n",
        "        caption=TEXT_PROMPT,\n",
        "        box_threshold=BOX_THRESHOLD,\n",
        "        text_threshold=TEXT_THRESHOLD,\n",
        "        device=device\n",
        "    )\n",
        "    annotated_frame = annotate(\n",
        "        image_source=image_source,\n",
        "        boxes=boxes,\n",
        "        logits=logits,\n",
        "        phrases=phrases\n",
        "    )\n",
        "\n",
        "    sv.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "rWR9PjB7v5gW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}